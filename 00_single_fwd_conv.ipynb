{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26febc30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.14.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/gpu/.venv/lib64/python3.12/site-packages/torch/_subclasses/functional_tensor.py:279: UserWarning: Failed to initialize NumPy: No module named 'numpy' (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  cpu = _conversion_method_template(device=torch.device(\"cpu\"))\n"
     ]
    }
   ],
   "source": [
    "import cudnn\n",
    "print(cudnn.__version__)\n",
    "import torch\n",
    "\n",
    "torch.manual_seed(42)\n",
    "assert torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a8aca43a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cudnn.backend_version(): 91002\n",
      "torch.version.cuda: 12.8\n"
     ]
    }
   ],
   "source": [
    "device: torch.cuda.device = torch.device(\"cuda\")\n",
    "print(f\"cudnn.backend_version(): {cudnn.backend_version()}\")\n",
    "print(f\"torch.version.cuda: {torch.version.cuda}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50469386",
   "metadata": {},
   "source": [
    "#### Initialise CUDNN Handle\n",
    "\n",
    "Initialises the library's context, it acts as an identifier for the current session with cuDNN. In the back-end, under underlying handle is explicitly passed to every subsequent library function that operates on GPU data. This provides the user with a means to explicitly control the library's functioning across multiple host threads, GPUs and CUDA streams. \n",
    "e.g. using cudaSetDevice can associate different physical GPUs with different host threads. With a different handle initialised in each host thread, the work from each different host thread will automatically run on different GPU devices\n",
    "\n",
    "The handle is used to determine on which GPU the kernel will be launched. The context is only associated wtih a single physical GPU device, however multiple handles can be initialised for a single physical GPU device.\n",
    "\n",
    "Front-end docs: https://docs.nvidia.com/deeplearning/cudnn/frontend/latest/developer/core-concepts.html#cudnn-handle \\\n",
    "Back-end docs: https://docs.nvidia.com/deeplearning/cudnn/backend/latest/developer/core-concepts.html#cudnn-handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c15ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "handle = cudnn.create_handle()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315a46b0",
   "metadata": {},
   "source": [
    "#### Initialise CUDNN Graph\n",
    "\n",
    "CUDNN provides a eclarative programming model, computation is defined via a graph of operations (on tensors), the CUDNN back-end handles how it is executed on the GPU device. Graphs are comprised of three main concepts, Operations, Execution Engines and Heuristics.\n",
    "\n",
    "*Operations* are a mathematical specification of the operations being executed.\\\n",
    "*Execution Engines*: TODO\\\n",
    "*Heuristics*: TODO\n",
    "\n",
    "\n",
    "Front-end docs: https://docs.nvidia.com/deeplearning/cudnn/frontend/v1.14.1/developer/graph-api.html#graphs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70807985",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = cudnn.pygraph(\n",
    "    handle=handle,\n",
    "    name=\"cudnn_graph_0\",\n",
    "    io_data_type=cudnn.data_type.HALF,\n",
    "    compute_data_type=cudnn.data_type.FLOAT,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232003d0",
   "metadata": {},
   "source": [
    "#### Define input tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf38157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensor in NHWC format then permute to NCHW\n",
    "X_gpu = torch.randn(8, 56, 56, 64, device=device, dtype=torch.float16).permute(\n",
    "    0, 3, 1, 2\n",
    ")\n",
    "W_gpu = torch.randn(32, 3, 3, 64, device=device, dtype=torch.float16).permute(\n",
    "    0, 3, 1, 2\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83191ff7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db30aff2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f1dff62",
   "metadata": {},
   "outputs": [],
   "source": [
    "cudnn.destroy_handle(handle)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
